{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "poetry_2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVsYe7S5XM5G"
      },
      "source": [
        "![](https://i.imgur.com/eBRPvWB.png)\n",
        "\n",
        "# Generowanie poezji za pomocą RNN i PyTorch sylabami\n",
        "\n",
        "[W tutorialu na rozgrzewkę](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) użyliśmy RNN, aby sklasyfikować nazwiska znak po znaku. Tym razem wygenerujemy tekst sylaba po sylabie.\n",
        "```\n",
        "Litwo! Ojczyzno moja! ty jesteś klucz wyziemu, \n",
        "To opugo cząciły tak lasu czeleta. \n",
        "Choć nie będzie mowę świeci się za tém, \n",
        "A Dozgon++ na Litwę przerzucił w okolicy, \n",
        "Dosyć się opicie przyciągnąć w pałacu; \n",
        "\n",
        "Tamdzini nawet mimo osobnych ogórki. \n",
        "Choć zwyciętunia, mimo pukle wyślą, \n",
        "Odemknął, wbiegł wyszedł, pewnie miłośnik łowił. \n",
        "Bo przekorza, i skrobiąc nabój do Warszawy. \n",
        "Dość co oddało plecie tak fawował, \n",
        "A tam się cukier wytaczać na nich wybująca. \n",
        "\n",
        "```\n",
        "\n",
        "Ok, możesz zadać sobie pytanie, czy ten tutorial jest rzeczywiście praktyczny? Czemu nie? Modele generatywne tego typu stanowią fundament tłumaczenia maszynowego, opisywania obrazów, generowania odpowiedzi na pytania i wielu innych zastowań.\n",
        "\n",
        "Zobacz [Sequence to Sequence Translation tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb) żeby nauczyć się więcej w tym temacie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WamMk47AXM5I"
      },
      "source": [
        "## Polecana lektura\n",
        "\n",
        "Zakładam, że jest już zainstalowany PyTorch, znasz Python'a, oraz znasz pojęcie Tensor'ów:\n",
        "\n",
        "* http://pytorch.org/ - instalacja PyTorch\n",
        "* [Deep Learning with PyTorch: A 60-minute Blitz](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) - Podstawy PyTorch\n",
        "* [jcjohnson's PyTorch examples](https://github.com/jcjohnson/pytorch-examples) przykłady wykorzystania PyTorch\n",
        "* [Introduction to PyTorch for former Torchies](https://github.com/pytorch/tutorials/blob/master/Introduction%20to%20PyTorch%20for%20former%20Torchies.ipynb) jeżeli znasz Lua Torch\n",
        "\n",
        "Trochę wiedzy o RNN:\n",
        "\n",
        "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) przykłady z życia wzięte\n",
        "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) RNN i LSTM w pigułce\n",
        "\n",
        "Zobacz także podobne tutoriale z serii:\n",
        "\n",
        "* [Classifying Names with a Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) używa RNN do klasyfikacji\n",
        "* [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb) opierając się na tym modelu, dodaje kategorię jako dane wejściowe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7PwzxO-hBJe"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqMD6uX0q25e"
      },
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import platform\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "from IPython.core.display import display, HTML\n",
        "import os\n",
        "import psutil\n",
        "import pickle\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import time, math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib as mpl\n",
        "mpl.style.use('default')\n",
        "mpl.style.use('bmh')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzZIXB-0zkmK"
      },
      "source": [
        "## Pobranie i instalacja stemmera"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEmB620tjBKi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bd6518d-0bda-4487-bec4-014ad9f858f9"
      },
      "source": [
        "!dpkg --add-architecture i386\n",
        "!apt-get -qq update\n",
        "!apt-get -qq install libc6:i386 libncurses5:i386 libstdc++6:i386"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package gcc-8-base:i386.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../0-gcc-8-base_8.4.0-1ubuntu1~18.04_i386.deb ...\n",
            "Unpacking gcc-8-base:i386 (8.4.0-1ubuntu1~18.04) ...\n",
            "Selecting previously unselected package libgcc1:i386.\n",
            "Preparing to unpack .../1-libgcc1_1%3a8.4.0-1ubuntu1~18.04_i386.deb ...\n",
            "Unpacking libgcc1:i386 (1:8.4.0-1ubuntu1~18.04) ...\n",
            "Selecting previously unselected package libc6:i386.\n",
            "Preparing to unpack .../2-libc6_2.27-3ubuntu1.3_i386.deb ...\n",
            "Unpacking libc6:i386 (2.27-3ubuntu1.3) ...\n",
            "Replacing files in old package libc6-i386 (2.27-3ubuntu1.3) ...\n",
            "Selecting previously unselected package libtinfo5:i386.\n",
            "Preparing to unpack .../3-libtinfo5_6.1-1ubuntu1.18.04_i386.deb ...\n",
            "Unpacking libtinfo5:i386 (6.1-1ubuntu1.18.04) ...\n",
            "Selecting previously unselected package libncurses5:i386.\n",
            "Preparing to unpack .../4-libncurses5_6.1-1ubuntu1.18.04_i386.deb ...\n",
            "Unpacking libncurses5:i386 (6.1-1ubuntu1.18.04) ...\n",
            "Selecting previously unselected package libstdc++6:i386.\n",
            "Preparing to unpack .../5-libstdc++6_8.4.0-1ubuntu1~18.04_i386.deb ...\n",
            "Unpacking libstdc++6:i386 (8.4.0-1ubuntu1~18.04) ...\n",
            "Selecting previously unselected package libgpm2:i386.\n",
            "Preparing to unpack .../6-libgpm2_1.20.7-5_i386.deb ...\n",
            "Unpacking libgpm2:i386 (1.20.7-5) ...\n",
            "Setting up gcc-8-base:i386 (8.4.0-1ubuntu1~18.04) ...\n",
            "Setting up libc6:i386 (2.27-3ubuntu1.3) ...\n",
            "Setting up libgcc1:i386 (1:8.4.0-1ubuntu1~18.04) ...\n",
            "Setting up libtinfo5:i386 (6.1-1ubuntu1.18.04) ...\n",
            "Setting up libgpm2:i386 (1.20.7-5) ...\n",
            "Setting up libstdc++6:i386 (8.4.0-1ubuntu1~18.04) ...\n",
            "Setting up libncurses5:i386 (6.1-1ubuntu1.18.04) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj3f3QRxoydK",
        "outputId": "7b363d87-4f7f-433f-94ad-1dce564405b4"
      },
      "source": [
        "!wget https://github.com/wojtekcz/ml_seminars/releases/download/v0.1/stemmer-2.0.3.tgz "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-11 21:38:57--  https://github.com/wojtekcz/ml_seminars/releases/download/v0.1/stemmer-2.0.3.tgz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/132335757/b410ba00-3bd5-11eb-8cfd-a34708ba2e16?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201211%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201211T213857Z&X-Amz-Expires=300&X-Amz-Signature=1a5411b25b5d8e2d9cfec2d7b30b18736953a2f86a50dad7c57ac997555c1d6b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=132335757&response-content-disposition=attachment%3B%20filename%3Dstemmer-2.0.3.tgz&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-12-11 21:38:57--  https://github-production-release-asset-2e65be.s3.amazonaws.com/132335757/b410ba00-3bd5-11eb-8cfd-a34708ba2e16?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201211%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201211T213857Z&X-Amz-Expires=300&X-Amz-Signature=1a5411b25b5d8e2d9cfec2d7b30b18736953a2f86a50dad7c57ac997555c1d6b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=132335757&response-content-disposition=attachment%3B%20filename%3Dstemmer-2.0.3.tgz&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.99.59\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.99.59|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2803075 (2.7M) [application/octet-stream]\n",
            "Saving to: ‘stemmer-2.0.3.tgz’\n",
            "\n",
            "stemmer-2.0.3.tgz   100%[===================>]   2.67M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-12-11 21:38:57 (22.9 MB/s) - ‘stemmer-2.0.3.tgz’ saved [2803075/2803075]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oitYYaiVjSRT"
      },
      "source": [
        "stemmer_path = Path('./') / 'stemmer-2.0.3.tgz'\n",
        "!tar xzf $stemmer_path"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGEJUx4GkFUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a388d7-e7ea-429e-cb60-4088bc6fe747"
      },
      "source": [
        "ls -lah bin"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8.2M\n",
            "drwxr-xr-x 2  501 staff 4.0K Apr 30  2018 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x 1 root root  4.0K Dec 11 21:38 \u001b[01;34m..\u001b[0m/\n",
            "-rw-r--r-- 1  501 staff 1.2K Apr 19  2018 changelog.txt\n",
            "-rwxr-xr-x 1  501 staff  629 Jan  2  2018 \u001b[01;32mdestem.sh\u001b[0m*\n",
            "-rw-r--r-- 1  501 staff 858K Nov 22  2017 stemmer2.dic\n",
            "-rwxrwxr-x 1  501 staff 3.7M Apr 19  2018 \u001b[01;32mstemmer.linux\u001b[0m*\n",
            "-rwxr-xr-x 1  501 staff 3.6M Apr 30  2018 \u001b[01;32mstemmer.macos\u001b[0m*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a-q1f8LFI5y"
      },
      "source": [
        "# Pobranie danych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_CBUv1zksmM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f6cc2e-d761-4aa6-b522-f32b9bb4a92f"
      },
      "source": [
        "dataset_path =   Path('data/rnn_generator'); dataset_path\n",
        "!mkdir -p $dataset_path\n",
        "!ls -la $dataset_path/\n",
        "tmp_path = dataset_path / 'tmp/'\n",
        "!mkdir -p $tmp_path"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Dec 11 21:39 .\n",
            "drwxr-xr-x 3 root root 4096 Dec 11 21:39 ..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q8kPT9ql7pa"
      },
      "source": [
        "## Pobranie datasetu Pan Tadeusz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrAOTu1GpAqf",
        "outputId": "dbd1abaf-13c6-4f86-f9fe-a96f335aacd1"
      },
      "source": [
        "!wget -P $dataset_path https://github.com/wojtekcz/ml_seminars/releases/download/v0.1/pan_tadeusz.txt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-11 21:39:03--  https://github.com/wojtekcz/ml_seminars/releases/download/v0.1/pan_tadeusz.txt\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/132335757/529e1a80-3bd8-11eb-9ad6-7de957bbb808?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201211%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201211T213903Z&X-Amz-Expires=300&X-Amz-Signature=afb650f03b7aa768605c70184163265eff684b249a1efe055c4adcd3cac16e5b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=132335757&response-content-disposition=attachment%3B%20filename%3Dpan_tadeusz.txt&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-12-11 21:39:03--  https://github-production-release-asset-2e65be.s3.amazonaws.com/132335757/529e1a80-3bd8-11eb-9ad6-7de957bbb808?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201211%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201211T213903Z&X-Amz-Expires=300&X-Amz-Signature=afb650f03b7aa768605c70184163265eff684b249a1efe055c4adcd3cac16e5b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=132335757&response-content-disposition=attachment%3B%20filename%3Dpan_tadeusz.txt&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.207.27\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.207.27|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 223706 (218K) [application/octet-stream]\n",
            "Saving to: ‘data/rnn_generator/pan_tadeusz.txt’\n",
            "\n",
            "pan_tadeusz.txt     100%[===================>] 218.46K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-12-11 21:39:03 (21.4 MB/s) - ‘data/rnn_generator/pan_tadeusz.txt’ saved [223706/223706]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMHb4Kjyl_3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ad19d5-762f-4296-bb1f-3b069ece4d18"
      },
      "source": [
        "ls -la $dataset_path/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 232\n",
            "drwxr-xr-x 3 root root   4096 Dec 11 21:39 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x 3 root root   4096 Dec 11 21:39 \u001b[01;34m..\u001b[0m/\n",
            "-rw-r--r-- 1 root root 223706 Dec 11 16:43 pan_tadeusz.txt\n",
            "drwxr-xr-x 2 root root   4096 Dec 11 21:39 \u001b[01;34mtmp\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoKLKM4tqfcI"
      },
      "source": [
        "## Opcjonalne pobranie innych datasetów"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruZQmzopoWBt"
      },
      "source": [
        "!wget -q -P $dataset_path https://github.com/wojtekcz/ml_seminars/releases/download/v0.1/witkacy_szewcy.txt"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua8TUPABot4y"
      },
      "source": [
        "!wget -q -P $dataset_path https://github.com/wojtekcz/ml_seminars/releases/download/v0.1/mickiewicz.txt"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y3kejFXmRku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab44e32-cb08-4952-a8c2-c7aaf7de9b0e"
      },
      "source": [
        "!ls -la $dataset_path/"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 1088\n",
            "drwxr-xr-x 3 root root   4096 Dec 11 21:39 .\n",
            "drwxr-xr-x 3 root root   4096 Dec 11 21:39 ..\n",
            "-rw-r--r-- 1 root root 725495 Dec 11 16:42 mickiewicz.txt\n",
            "-rw-r--r-- 1 root root 223706 Dec 11 16:43 pan_tadeusz.txt\n",
            "drwxr-xr-x 2 root root   4096 Dec 11 21:39 tmp\n",
            "-rw-r--r-- 1 root root 145132 Dec 11 16:42 witkacy_szewcy.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnHlimami7b1"
      },
      "source": [
        "## Załadowanie bibliotek"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8KbaMvrXM5J"
      },
      "source": [
        "## Preprocessing korpusu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80ZjgATAXsVn"
      },
      "source": [
        "dataset_path = Path('data/rnn_generator'); dataset_path\n",
        "tmp_path = dataset_path / 'tmp/'\n",
        "!mkdir -p $tmp_path"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPFXCZguyyXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9754d76-6b84-4bbf-de79-936af66b4701"
      },
      "source": [
        "ls -lah $dataset_path/"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 1.1M\n",
            "drwxr-xr-x 3 root root 4.0K Dec 11 21:39 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x 3 root root 4.0K Dec 11 21:39 \u001b[01;34m..\u001b[0m/\n",
            "-rw-r--r-- 1 root root 709K Dec 11 16:42 mickiewicz.txt\n",
            "-rw-r--r-- 1 root root 219K Dec 11 16:43 pan_tadeusz.txt\n",
            "drwxr-xr-x 2 root root 4.0K Dec 11 21:39 \u001b[01;34mtmp\u001b[0m/\n",
            "-rw-r--r-- 1 root root 142K Dec 11 16:42 witkacy_szewcy.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eTRgNsMyyXD"
      },
      "source": [
        "fn_corpus_char = dataset_path/'pan_tadeusz.txt'\n",
        "fn_corpus_caps = dataset_path/'pan_tadeusz.caps1.txt'\n",
        "fn_corpus_syl = dataset_path/'pan_tadeusz.syl1.txt'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wuZu6mE057C"
      },
      "source": [
        "Plik wejściowy (korpus) to duży plik tekstowy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaV5Ly3R_tzj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9526fff-4f29-43d6-b673-2895ddc3df7f"
      },
      "source": [
        "!head -n 21 $fn_corpus_char"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿KSIĘGA PIÉRWSZA.\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "GOSPODARSTWO.\r\n",
            "\r\n",
            "\r\n",
            "TREŚĆ.\r\n",
            "\r\n",
            "    Powrot panicza -- Spotkanie się piérwsze w pokoiku, drugie u\r\n",
            "    stołu -- Ważna Sędziego nauka o grzeczności -- Podkomorzego uwagi\r\n",
            "    polityczne nad modami -- Początek sporu o Kusego i Sokoła -- Żale\r\n",
            "    Wojskiego -- Ostatni Woźny Trybunału -- Rzut oka na ówczesny stan\r\n",
            "    polityczny Litwy i Europy.\r\n",
            "\r\n",
            "\r\n",
            "  Litwo! Ojczyzno moja! ty jesteś jak zdrowie;\r\n",
            "Ile cię trzeba cenić, ten tylko się dowie\r\n",
            "Kto cię stracił. Dziś piękność twą w całéj ozdobie\r\n",
            "Widzę i opisuję, bo tęsknię po tobie.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKPynzs-yyW-"
      },
      "source": [
        "### Tokenizacja wielkich liter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enPpQXzj_oj6"
      },
      "source": [
        "Zamieniamy duże litery na małe dodając tokeny `_up_` (dla wyrazów pisanych wielkimi literami) lub `_cap_` (dla wyrazów pisanych z wielkiej litery)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJtiF_yZiA-z"
      },
      "source": [
        "def do_caps(ss):\n",
        "  TOK_UP,TOK_CAP = ' _up_ ', ' _cap_ '\n",
        "  res = []\n",
        "  re_word = re.compile('\\w')\n",
        "  for s in re.findall(r'\\w+|\\W+', ss):\n",
        "      res += ([TOK_UP,s.lower()] if (s.isupper() and (len(s)>2))\n",
        "              else [TOK_CAP,s.lower()] if s.istitle()\n",
        "              else [s.lower()])\n",
        "  return ''.join(res)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5cAVEtNiC9X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "485822af-ac45-45b0-c366-55d7eeba0d7e"
      },
      "source": [
        "corpus_tmp = fn_corpus_char.open('r').read()\n",
        "corpus_tmp = do_caps(corpus_tmp)\n",
        "fn_corpus_caps.open('w').write(corpus_tmp)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "246668"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJlxNo8q-quJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "533a855c-840c-451a-9445-5853270c793b"
      },
      "source": [
        "!head -n 21 $fn_corpus_caps"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿ _up_ księga  _up_ piérwsza.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " _up_ gospodarstwo.\n",
            "\n",
            "\n",
            " _up_ treść.\n",
            "\n",
            "     _cap_ powrot panicza --  _cap_ spotkanie się piérwsze w pokoiku, drugie u\n",
            "    stołu --  _cap_ ważna  _cap_ sędziego nauka o grzeczności --  _cap_ podkomorzego uwagi\n",
            "    polityczne nad modami --  _cap_ początek sporu o  _cap_ kusego i  _cap_ sokoła --  _cap_ żale\n",
            "     _cap_ wojskiego --  _cap_ ostatni  _cap_ woźny  _cap_ trybunału --  _cap_ rzut oka na ówczesny stan\n",
            "    polityczny  _cap_ litwy i  _cap_ europy.\n",
            "\n",
            "\n",
            "   _cap_ litwo!  _cap_ ojczyzno moja! ty jesteś jak zdrowie;\n",
            " _cap_ ile cię trzeba cenić, ten tylko się dowie\n",
            " _cap_ kto cię stracił.  _cap_ dziś piękność twą w całéj ozdobie\n",
            " _cap_ widzę i opisuję, bo tęsknię po tobie.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbMBLa_nxp2b"
      },
      "source": [
        "### Podział korpusu na sylaby"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OONrdDyb-mS6"
      },
      "source": [
        "Dzielimy korpus na sylaby programem `stemmer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efaGn-TLyyXG"
      },
      "source": [
        "platform_suffixes = {'Linux': 'linux', 'Darwin': 'macos'}\n",
        "platform_suffix = platform_suffixes[platform.system()]\n",
        "stemmer_bin = f'LD_PRELOAD=\"\" bin/stemmer.{platform_suffix}'"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3qYJ20EyyXI"
      },
      "source": [
        "# !$stemmer_bin -h"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGbQYAUYyyXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ec28f1-afb4-4508-d987-c27bd844e6bd"
      },
      "source": [
        "!$stemmer_bin -s 7683 -v -d bin/stemmer2.dic -i $fn_corpus_caps -o $fn_corpus_syl"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stemmer 2.0.3 2018-04-19 (Linux i386)\n",
            "For Korrida database, spellchecker and hyphenator copyright (C) 1993-2018 Wojciech Czarnowski\n",
            "For Stemmer copyright (C) 2018 Krzysztof Wolk and Wojciech Czarnowski\n",
            "Wojciech Czarnowski: wojtek.czarnowski@gmail.com, +48(608)202-272\n",
            "Krzysztof Wolk: krz.wolk@gmail.com, +48(606)918-623\n",
            "\n",
            "Dictionary: \"bin/stemmer2.dic\"\n",
            "Input file: \"data/rnn_generator/pan_tadeusz.caps1.txt\"\n",
            "Output file: \"data/rnn_generator/pan_tadeusz.syl1.txt\"\n",
            "Stem number: \"7683\"\n",
            "\n",
            "Stemming options:\n",
            "  StemNiePrefix     : Yes\n",
            "  StemExtraPrefixes : Yes\n",
            "  StemPrefixesInRoot: No\n",
            "\n",
            "Syllable division options:\n",
            "  DivideWords          : Yes\n",
            "  DivideWithDictionary : Yes\n",
            "  DivideAlgorithmically: Yes\n",
            "  DivideUknkownWords   : Yes\n",
            "  divideAfterChar      : 1\n",
            "\n",
            "Stemming formatting options:\n",
            "  StemInSuffix       : No\n",
            "  ShowPOSInfo        : No\n",
            "  ShowExtraPOSInfo   : No\n",
            "\n",
            "  ShowGroupCode      : No\n",
            "  ShowBaseSuffixCodes: No\n",
            "  ShowSuffixCode     : No\n",
            "\n",
            "  stemDelimiterStr   : \"++ --\"\n",
            "  codeDelimiterStr   : \"@@\"\n",
            "\n",
            "StemFile(fileInPath: \"data/rnn_generator/pan_tadeusz.caps1.txt\", fileOutPath: \"data/rnn_generator/pan_tadeusz.syl1.txt\")\n",
            "Finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rurUv5wf_135",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5847bee5-3eee-44de-9b2a-96557d90f599"
      },
      "source": [
        "!head -n 21 $fn_corpus_syl"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " _up_ księ++ --ga  _up_ pié++ --rwsza.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " _up_ go++ --s++ --po++ --dar++ --stwo.\n",
            "\n",
            "\n",
            " _up_ treść.\n",
            "\n",
            "     _cap_ po++ --wrot pa++ --ni++ --cza --  _cap_ spot++ --ka++ --nie się pié++ --rwsze w po++ --koi++ --ku, dru++ --gie u\n",
            "    sto++ --łu --  _cap_ waż++ --na  _cap_ sę++ --dzie++ --go na++ --u++ --ka o grze++ --cz++ --no++ --ści --  _cap_ pod++ --ko++ --mo++ --rze++ --go u++ --wa++ --gi\n",
            "    po++ --li++ --ty++ --cz++ --ne nad mo++ --da++ --mi --  _cap_ po++ --czą++ --tek spo++ --ru o  _cap_ ku++ --se++ --go i  _cap_ so++ --ko++ --ła --  _cap_ ża++ --le\n",
            "     _cap_ woj++ --skie++ --go --  _cap_ o++ --sta++ --t++ --ni  _cap_ woź++ --ny  _cap_ try++ --bu++ --na++ --łu --  _cap_ rzut oka na ów++ --czes++ --ny stan\n",
            "    po++ --li++ --ty++ --cz++ --ny  _cap_ lit++ --wy i  _cap_ eu++ --ro++ --py.\n",
            "\n",
            "\n",
            "   _cap_ lit++ --wo!  _cap_ oj++ --czyz++ --no mo++ --ja! ty je++ --s++ --teś jak zdro++ --wie;\n",
            " _cap_ ile cię trze++ --ba ce++ --nić, ten tyl++ --ko się do++ --wie\n",
            " _cap_ kto cię stra++ --cił.  _cap_ dziś pięk++ --ność twą w całéj o++ --zdo++ --bie\n",
            " _cap_ wi++ --dzę i o++ --pi++ --su++ --ję, bo tęs++ --knię po to++ --bie.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpf6QibJyyXL"
      },
      "source": [
        "### Załadowanie do pamięci i tokenizacja"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSyGzgVD057R"
      },
      "source": [
        "Ładujemy korpus do pamięci i tokenizujemy. Tworzymy też listę wszystkich tokenów `all_tokens`. Mamy już specjalne tokeny `_cap_` i `_up_`, zamieniamy znaki końca lini na token `_eol_` i dodajemy token `_unk_` na wypadek, gdybyśmy użyli sylaby (tokena), który nie wystąpił wcześniej w korpusie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9H83p3sXM5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e5c2eca-d4b1-45f3-8f51-e7cb8318be1d"
      },
      "source": [
        "file = open(fn_corpus_syl).read()\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_len = 398396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maVrQMtZyyXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c44c5b-afbf-447c-8a6d-80fee6214900"
      },
      "source": [
        "# taken from fastai/text.py\n",
        "\n",
        "# remove +,- chars from punctuation set to keep syllables e.g.'--PO++' intact\n",
        "# remove _ char to keep tokens intact\n",
        "punctuation=re.sub('[_\\+-]', '', string.punctuation)\n",
        "re_tok = re.compile(f'([{punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
        "\n",
        "def tokenize(s, repl_unk=True): \n",
        "  strings = re_tok.sub(r' \\1 ', s).replace('\\n', ' _eol_ ').split()\n",
        "  if repl_unk:\n",
        "    strings = [str2tok(s) for s in strings]\n",
        "  return strings\n",
        "\n",
        "file_tok = tokenize(file, repl_unk=False); len(file_tok), file_tok[:8]\n",
        "file_tok_len = len(file_tok)\n",
        "\n",
        "spec_tokens = ['_unk_', '_eol_', '_cap_', '_up_']\n",
        "\n",
        "all_tokens = []\n",
        "all_tokens.extend(spec_tokens)\n",
        "all_tokens.extend(sorted(list(set(file_tok))))\n",
        "n_tokens = len(all_tokens); print(n_tokens, all_tokens[:50])\n",
        "\n",
        "tok2idx_dict = {tok: idx for (idx, tok) in enumerate(all_tokens)}\n",
        "\n",
        "def str2tok(str) -> int:\n",
        "  return str if tok2idx_dict.get(str, 0) else all_tokens[0]\n",
        "\n",
        "def tok2idx(tok) -> int:\n",
        "  return tok2idx_dict.get(tok, 0)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5059 ['_unk_', '_eol_', '_cap_', '_up_', '!', '\"', '%', \"'\", '(', ')', ',', '--', '--a++', '--aczéj', '--ał', '--b++', '--ba', '--ba++', '--bach', '--baj++', '--bak', '--ban', '--ban++', '--bar++', '--barz', '--baw', '--baw++', '--bał', '--baż', '--bcem', '--be++', '--bek', '--bel', '--bel++', '--belg', '--bem', '--ber++', '--bez++', '--beł', '--bi', '--bi++', '--bia', '--bia++', '--biad', '--biar++', '--biać', '--biał', '--bic', '--bie', '--bie++']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaNxdcoMADVS"
      },
      "source": [
        "Przyda nam się funkcja do zakodowania dowolnego tekstu na listę zsylabizowanych tokenów:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDQqpRYzxT4L"
      },
      "source": [
        "def str2syl2tok(text):  \n",
        "  fn_tmp_text_caps = Path(tmp_path / 'tmp_text_caps1.txt')\n",
        "  fn_tmp_text_syl = Path(tmp_path / 'tmp_text_syl1.txt')\n",
        "  \n",
        "  text = do_caps(text)\n",
        "  fn_tmp_text_caps.open('w').write(text)\n",
        "  \n",
        "  !$stemmer_bin -s 7683 -d bin/stemmer2.dic -i $fn_tmp_text_caps -o $fn_tmp_text_syl\n",
        "  \n",
        "  text_syl = fn_tmp_text_syl.open('r').read()\n",
        "  \n",
        "  # kill last \\n eol char possibly added by stemmer\n",
        "  if text_syl[-1] == '\\n':\n",
        "    text_syl = text_syl[:-1]\n",
        "\n",
        "  text_tok = tokenize(text_syl, repl_unk=True)\n",
        "    \n",
        "  return text_tok"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwnDdj75paVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb9cf84-b00c-4088-da7d-8807b4e31373"
      },
      "source": [
        "tekst = 'LITWO! Ojczyzno moja!\\nTy jesteś jak zdrowie.\\nIle cię trzeba cenić ble ble '\n",
        "tekst_tok = str2syl2tok(tekst); print(tekst_tok)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['_up_', 'lit++', '--wo', '!', '_cap_', 'oj++', '--czyz++', '--no', 'mo++', '--ja', '!', '_eol_', '_cap_', 'ty', 'je++', '--s++', '--teś', 'jak', 'zdro++', '--wie', '.', '_eol_', '_cap_', 'ile', 'cię', 'trze++', '--ba', 'ce++', '--nić', '_unk_', '_unk_']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAUKI80V6CvK"
      },
      "source": [
        "Funkcje pomocnicze do zdekodowania listy tokenów na tekst:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzAAi_95yyXq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "189e04dc-5385-41a1-c613-770914cceaca"
      },
      "source": [
        "def syl2str(a_list, delim='/'): \n",
        "  s = ' '.join(a_list)\n",
        "  \n",
        "  repl_list = [\n",
        "      ('++ --', delim), \n",
        "  ]\n",
        "  for repl in repl_list:\n",
        "    s = s.replace(repl[0], repl[1])\n",
        "  \n",
        "  return s\n",
        "\n",
        "print(syl2str(tekst_tok))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_up_ lit/wo ! _cap_ oj/czyz/no mo/ja ! _eol_ _cap_ ty je/s/teś jak zdro/wie . _eol_ _cap_ ile cię trze/ba ce/nić _unk_ _unk_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUs-bnu5q2FF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d7bf7c-314b-45fd-d323-9b609b3c73b8"
      },
      "source": [
        "def decode_tokens(e_str):\n",
        "  # decode _eol_, _cap_ and _up_\n",
        "  # leave _unk_ token alone\n",
        "  e_syl = e_str.split(' ')\n",
        "  e_syl2 = []\n",
        "\n",
        "  cap = False; up = False\n",
        "\n",
        "  for syl in e_syl:\n",
        "    if syl == '_eol_': syl = '\\n'\n",
        "\n",
        "    if syl not in ['_cap_', '_up_']:\n",
        "      if cap == True: syl = syl.title(); cap = False\n",
        "      if up == True: syl = syl.upper(); up = False        \n",
        "      e_syl2.append(syl)\n",
        "\n",
        "    if syl == '_cap_': cap = True\n",
        "    if syl == '_up_': up = True\n",
        "\n",
        "  return ' '.join(e_syl2)\n",
        "\n",
        "print(decode_tokens(syl2str(tekst_tok, delim=''))[:300])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LITWO ! Ojczyzno moja ! \n",
            " Ty jesteś jak zdrowie . \n",
            " Ile cię trzeba cenić _unk_ _unk_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqbQLawfrV-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f442c316-d1a1-43b4-b396-a2bd8896d180"
      },
      "source": [
        "def fix_punctuation(s): \n",
        "  repl_list = [\n",
        "      ('\\n ', '\\n'), \n",
        "      (' ,', ','),\n",
        "      (' .', '.'),\n",
        "      (' !', '!'),\n",
        "      (' ?', '?'),\n",
        "      (' ;', ';'),\n",
        "      ('( ', '('),\n",
        "      (' )', ')'),\n",
        "      (' «', '«'),\n",
        "      ('» ', '»'),\n",
        "      (' :', ':')\n",
        "  ]\n",
        "  \n",
        "  for repl in repl_list:\n",
        "    s = s.replace(repl[0], repl[1])\n",
        "  \n",
        "  return s\n",
        "\n",
        "print(fix_punctuation(decode_tokens(syl2str(tekst_tok, delim='')))[:300])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LITWO! Ojczyzno moja! \n",
            "Ty jesteś jak zdrowie. \n",
            "Ile cię trzeba cenić _unk_ _unk_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2VJHqQmCyuT"
      },
      "source": [
        "Sformatujmy zdekodowany tekst w HTML i zaznaczmy na czerwono sylaby, z których nie dało się skleić słów."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFj5nGp5yyYI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8f03124e-bedf-4167-9a38-f85dba951236"
      },
      "source": [
        "class X(str):\n",
        "    def rpl(self, p, c='lightgray'):\n",
        "        return X(self.replace(p, f'<font color=\"{c}\">{p}</font>'))\n",
        "    def rpl2(self, p, p2):\n",
        "        return X(self.replace(p, p2))\n",
        "      \n",
        "def format_html(e_str):\n",
        "  return X(e_str).rpl('/').rpl('--', c='red').rpl('++', c='red').rpl2('\\n', '\\n<br/>')\n",
        "\n",
        "e_str = fix_punctuation(decode_tokens(syl2str(tekst_tok, delim='')))[:400]\n",
        "e_html = format_html(e_str); display(HTML(e_html))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "LITWO! Ojczyzno moja! \n",
              "<br/>Ty jesteś jak zdrowie. \n",
              "<br/>Ile cię trzeba cenić _unk_ _unk_"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnmXGTva5nw-"
      },
      "source": [
        "## Przygotowanie treningu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cIY0S4g0gZo"
      },
      "source": [
        "### GPU?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hTjsshick9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4842acfe-f0ee-4604-d40d-f6160370011a"
      },
      "source": [
        "USE_GPU = torch.cuda.is_available(); \n",
        "# USE_GPU = False; \n",
        "\n",
        "print(f'USE_GPU={USE_GPU}')\n",
        "\n",
        "def to_gpu(x, *args, **kwargs):\n",
        "    return x.cuda(*args, **kwargs) if USE_GPU else x"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "USE_GPU=True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5cFLx6WXM5X"
      },
      "source": [
        "### Budowa sieci rekurencyjnej\n",
        "\n",
        "Ten model przyjmie jako wejściie token dla kroku $ t _ {- 1} $ i ma wyprowadzić następny token $ t $. Istnieją trzy warstwy - jedna warstwa liniowa, która koduje znak wejściowy do stanu wewnętrznego, jedna warstwa GRU (która może sama mieć wiele warstw), która działa na tym stanie wewnętrznym i stanie ukrytym, oraz warstwa dekodera, która wyprowadza rozkład prawdopodobieństwa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ8chQcVXM5X"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        input = self.encoder(input.view(1, -1))\n",
        "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
        "        output = self.decoder(output.view(1, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return Variable(to_gpu(torch.zeros(self.n_layers, 1, self.hidden_size)))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62XSRFgkXM5Z"
      },
      "source": [
        "### Tensory wejściowe i docelowe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL_XOG-fXM5T"
      },
      "source": [
        "Aby stworzyć 'wejścia' z tego dużego ciągu danych, podzielimy go na kawałki po 400 sylab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zwmRSAHXM5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "738ec601-581e-4777-baa3-c4fa79afeaa5"
      },
      "source": [
        "chunk_len = 400\n",
        "\n",
        "def random_chunk():\n",
        "    start_index = random.randint(0, file_tok_len - chunk_len -1)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file_tok[start_index:end_index]\n",
        "  \n",
        "n_samples = file_tok_len // chunk_len; n_samples, file_tok_len"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(198, 79544)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaOPvn0rXM5Z"
      },
      "source": [
        "Każdy 'kawałek' zostanie przekształcony w tensor, a dokładnie w `LongTensor` (używany do wartości całkowitych), poprzez przepuszczenie wszystkich tokenów ciągu i wyszukiwanie indeksu każdej sylaby w `all_tokens`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0H2nwMMXM5a"
      },
      "source": [
        "# Turn token list into list of longs\n",
        "def tok_tensor(token_list):\n",
        "    tensor = torch.zeros(len(token_list)).long()\n",
        "    for c in range(len(token_list)):\n",
        "        tensor[c] = tok2idx(token_list[c])\n",
        "    \n",
        "    return Variable(to_gpu(tensor))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrTEI7EjyyX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7956feca-c2e8-4c16-a907-dac72f0476fe"
      },
      "source": [
        "tekst = 'Litwo! Ojczyzno moja! ty jesteś jak zdrowie;'\n",
        "tekst_tok = str2syl2tok(tekst)\n",
        "print(tekst_tok)\n",
        "print(tok_tensor(tekst_tok))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['_cap_', 'lit++', '--wo', '!', '_cap_', 'oj++', '--czyz++', '--no', 'mo++', '--ja', '!', 'ty', 'je++', '--s++', '--teś', 'jak', 'zdro++', '--wie', ';']\n",
            "tensor([2721, 3565, 2396,    4, 2721, 3776,  435, 1424, 3642,  865,    4, 4468,\n",
            "        3365, 1738, 2151, 3354, 4810, 2332, 2716], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Su49JvFXM5d"
      },
      "source": [
        "Wreszcie możemy zmontować parę tensorów wejściowych i docelowych do treningu, z losowego kawałka. Wejściem zostaną wszystkie tokeny * aż do przedostatniego*, a celem (targetem) będą wszystkie tokeny * od drugiego*. Jeśli więc nasz kawałek to \"abc\", wejście będzie odpowiadać \"ab\", podczas gdy cel to \"bc\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLzzsbTRXM5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e931a7a2-4981-4869-8ff2-b6b3f477f3f7"
      },
      "source": [
        "def random_training_set():  \n",
        "    chunk = random_chunk()\n",
        "    inp = tok_tensor(chunk[:-1])\n",
        "    target = tok_tensor(chunk[1:])\n",
        "    return inp, target\n",
        "  \n",
        "inp, target = random_training_set(); inp[:9], target[:9]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([2315,   11, 2721, 4488, 4216, 2593, 4708, 2108,  364], device='cuda:0'),\n",
              " tensor([  11, 2721, 4488, 4216, 2593, 4708, 2108,  364, 4107], device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSJ_szQTXM5f"
      },
      "source": [
        "### Ewaluacja wyników\n",
        "\n",
        "Aby ocenić sieć, będziemy podawać po jednym tokenie na raz, wykorzystywać wyjścia sieci jako rozkład prawdopodobieństwa dla następnego znaku i powtarzać. Aby rozpocząć generowanie, przekazujemy ciąg wstępny, aby rozpocząć budowanie stanu ukrytego, z którego następnie generujemy po jednym tokenie na raz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ecqC4rWXM5f"
      },
      "source": [
        "def evaluate(prime_tokl=[all_tokens[1]], predict_len=100, temperature=0.8):\n",
        "    hidden = decoder.init_hidden()\n",
        "    prime_input = tok_tensor(prime_tokl)\n",
        "    predicted = list(prime_tokl)  # need a copy of the list\n",
        "\n",
        "    # Use priming token list to \"build up\" hidden state\n",
        "    for p in range(len(prime_tokl) - 1):\n",
        "        _, hidden = decoder(prime_input[p], hidden)\n",
        "    inp = prime_input[-1]\n",
        "    \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = decoder(inp, hidden)\n",
        "        \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        \n",
        "        # in pytorch 0.4.0 max, min fail if there are Infs or nans\n",
        "        # https://github.com/pytorch/pytorch/issues/6996\n",
        "        # in all pytorch versions multinomial fails if there are Infs or nans\n",
        "        # https://github.com/pytorch/pytorch/issues/871\n",
        "        # temp fix, kill Infs and nans\n",
        "        # https://discuss.pytorch.org/t/how-to-set-inf-in-tensor-variable-to-0/10235\n",
        "        output_dist[output_dist == float(\"Inf\")] = 0\n",
        "        output_dist[output_dist == float(\"nan\")] = 0\n",
        "        \n",
        "        top_i = torch.multinomial(output_dist, 1)[0].item()\n",
        "        \n",
        "        # Add predicted token to the list and use as next input\n",
        "        predicted_token = all_tokens[top_i]\n",
        "        predicted.append(predicted_token)\n",
        "        inp = tok_tensor([predicted_token])\n",
        "\n",
        "    return predicted"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZY0fPgEXM5h"
      },
      "source": [
        "## Trening sieci"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pM5T97tXM5h"
      },
      "source": [
        "Funkcja pomocnicza do wydrukowania upływającego czasu:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQnLeX-TXM5h"
      },
      "source": [
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pntPTWEXM5i"
      },
      "source": [
        "Główna funkcja treningowa:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKb7-MeXXM5j"
      },
      "source": [
        "def train(inp, target):\n",
        "    hidden = decoder.init_hidden()\n",
        "    decoder.zero_grad()\n",
        "    loss = 0\n",
        "\n",
        "    for c in range(chunk_len):\n",
        "        output, hidden = decoder(inp[c], hidden)\n",
        "        loss += criterion(output, target[c].expand(1))\n",
        "\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / chunk_len"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EiZtEsA0571"
      },
      "source": [
        "Opcjonalny monitoring postępu treningu:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpYR9j3qyyYF"
      },
      "source": [
        "USE_VISDOM = False\n",
        "\n",
        "vis = None\n",
        "if USE_VISDOM:\n",
        "    import visdom\n",
        "    vis = visdom.Visdom(port=8890)\n",
        "\n",
        "def vis_update_line_chart(vis, name, x, y, first_step):\n",
        "    if not USE_VISDOM: return\n",
        "    vis.line(Y=np.array([y]), X=np.array([x]), win=name, opts=dict(title=name),\n",
        "             update=None if first_step else 'append')\n",
        "\n",
        "def vis_update_text_win(vis, name, text):\n",
        "    if not USE_VISDOM: return\n",
        "    vis.text(text, win=name, opts=dict(title=name), append=False)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sjJFcYJDzZ2"
      },
      "source": [
        "Wskaźnik liczby sylab, z których nie dało się skleić słów:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY2U1vt4yyYK"
      },
      "source": [
        "def bad_words(e_syl): e_str = syl2str(e_syl); return (e_str.count('++') + e_str.count('--')) / len(e_syl)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyo69fakXM5k"
      },
      "source": [
        "Następnie definiujemy parametry treningowe i rozpoczynamy trening:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILThkaRPXM5k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53bd76a9-6d87-4dc5-85f2-a5b084ed169f"
      },
      "source": [
        "n_epochs = 6\n",
        "n_iters = n_epochs * n_samples\n",
        "print_every = n_samples // 2\n",
        "plot_every = n_samples // 4\n",
        "hidden_size = 500\n",
        "n_layers = 3\n",
        "lr = 0.001\n",
        "\n",
        "decoder = RNN(n_tokens, hidden_size, n_tokens, n_layers)\n",
        "if USE_GPU:\n",
        "    decoder.cuda()\n",
        "print(decoder, flush=True)\n",
        "\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if USE_GPU:\n",
        "    criterion.cuda()\n",
        "\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "all_bw = []\n",
        "bw_avg = 0\n",
        "\n",
        "iterable = range(1, n_iters + 1)\n",
        "tqdm_ = tqdm(iterable, '', leave=False, dynamic_ncols=True, mininterval=1.0, ascii=True, miniters=1)\n",
        "first_step = True\n",
        "\n",
        "prime_tok = str2syl2tok('Litwo! Ojczyzno moja!')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN(\n",
            "  (encoder): Embedding(5059, 500)\n",
            "  (gru): GRU(500, 500, num_layers=3)\n",
            "  (decoder): Linear(in_features=500, out_features=5059, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1188 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6y2kdZUj2bw"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "for it in tqdm_:\n",
        "    epoch = 1 + it // n_samples\n",
        "    loss = train(*random_training_set())       \n",
        "    loss_avg += loss\n",
        "\n",
        "    # current loss chart\n",
        "    vis_update_line_chart(vis, 'loss', it, loss, it == 1)\n",
        "\n",
        "    # bad words    \n",
        "    bw = bad_words(evaluate(prime_tok, 100))\n",
        "    bw_avg += bw\n",
        "\n",
        "    # current bad words chart\n",
        "    vis_update_line_chart(vis, 'bad_words', it, bw, it == 1)\n",
        "    \n",
        "    # progress_bar\n",
        "    tqdm_.set_postfix({'epoch': f'{epoch}/{n_epochs}', 'loss': loss, 'bw': bw})\n",
        "    text = f'&nbsp;<font color=\"red\">{tqdm_}</font>'\n",
        "    vis_update_text_win(vis, 'progress_bar', text)\n",
        "\n",
        "    if it % print_every == 0:\n",
        "        e_syl = evaluate(prime_tok, 1000)\n",
        "        e_bw = bad_words(e_syl)\n",
        "        stats_str = '\\n[%s (%d %d %d%%) loss=%.4f bw=%.4f]' % (time_since(start), epoch, it, it / n_iters * 100, loss, e_bw)\n",
        "        print(stats_str)\n",
        "        \n",
        "        e_str = fix_punctuation(decode_tokens(syl2str(e_syl, delim='')))\n",
        "        e_html = format_html(e_str); display(HTML(e_html))\n",
        "        print(flush=True)        \n",
        "        \n",
        "        text = f'<b>{stats_str}</b><br />{e_html}'\n",
        "        vis_update_text_win(vis, 'evaluation', text)\n",
        "        \n",
        "        e_syl_path = tmp_path / 'e_syl.txt'\n",
        "        e_syl_path.open('w').write(' '.join(e_syl))\n",
        "\n",
        "    if it % plot_every == 0:\n",
        "        vis_update_line_chart(vis, 'loss_avg', it, loss_avg / plot_every, first_step)\n",
        "        vis_update_line_chart(vis, 'bad_words_avg', it, bw_avg / plot_every, first_step)\n",
        "        all_bw.append(bw)\n",
        "        bw_avg = 0\n",
        "        first_step = False\n",
        "        all_losses.append(loss_avg / plot_every)\n",
        "        loss_avg = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXrPD94SXM5m"
      },
      "source": [
        "### Kreślenie wartości straty\n",
        "\n",
        "Wykreślanie historii straty z `all_losses` pokazuje uczenie sieci:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meKCxPo3XM5n"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WabkwPqZ5WlH"
      },
      "source": [
        "### Zapis i odczyt sieci"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtK-9BuuB-GX"
      },
      "source": [
        "#### Zapisanie sieci"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79HBmGUecvPh"
      },
      "source": [
        "dataset_path = Path('data/rnn_generator'); dataset_path\n",
        "pretrained_path = dataset_path / 'trained/'\n",
        "!mkdir -p $pretrained_path"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSA3Utpc6dXV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8860230-f656-4672-a00b-3a117f54e843"
      },
      "source": [
        "# n_epochs=15\n",
        "\n",
        "ALLTOKS, MODEL = ['all_tokens', 'model']\n",
        "fn_pan_tadeusz = {ALLTOKS: f'all_tokens.pan_tadeusz.json', \n",
        "                  MODEL: f'pan_tadeusz.h{hidden_size}.l{n_layers}.e{n_epochs}.gpu.torch'}\n",
        "fn_dict = fn_pan_tadeusz; fn_dict"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'all_tokens': 'all_tokens.pan_tadeusz.json',\n",
              " 'model': 'pan_tadeusz.h500.l3.e6.gpu.torch'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ-j5_4I6d0g"
      },
      "source": [
        "# save all_tokens\n",
        "all_tokens_path = pretrained_path / fn_dict[ALLTOKS]\n",
        "all_tokens_path.write_text(json.dumps(all_tokens,indent=4))\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# save model\n",
        "model_path = pretrained_path / fn_dict[MODEL]\n",
        "torch.save(decoder, model_path)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt8_7GshWi4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d6aa8c2-d637-48ce-af28-a6de9317b014"
      },
      "source": [
        "ls -lah $tmp_path"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 37M\n",
            "drwxr-xr-x 2 root root 4.0K Dec 11 21:41 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x 4 root root 4.0K Dec 11 21:41 \u001b[01;34m..\u001b[0m/\n",
            "-rw-r--r-- 1 root root  37M Dec 11 21:41 pan_tadeusz.h500.l3.e6.gpu.torch\n",
            "-rw-r--r-- 1 root root   35 Dec 11 21:40 tmp_text_caps1.txt\n",
            "-rw-r--r-- 1 root root   56 Dec 11 21:40 tmp_text_syl1.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHbGgVULB_cp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6d9efbc-2f97-4018-a053-800aa84fea45"
      },
      "source": [
        "decoder.state_dict"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.state_dict of RNN(\n",
              "  (encoder): Embedding(5059, 500)\n",
              "  (gru): GRU(500, 500, num_layers=3)\n",
              "  (decoder): Linear(in_features=500, out_features=5059, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BalhBXFKfErp"
      },
      "source": [
        "#### Załadowanie sieci"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk7EWQXmgfdB"
      },
      "source": [
        "dataset_path = Path('data/rnn_generator'); dataset_path\n",
        "pretrained_path = dataset_path / 'trained/'\n",
        "!mkdir -p $pretrained_path"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it1JwumPZkBD"
      },
      "source": [
        "Model Pan Tedeusz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttGGVZHRYlzc"
      },
      "source": [
        "!wget -q -P $pretrained_path https://github.com/wojtekcz/ml_seminars/releases/download/v0.1/pan_tadeusz.h500.l3.e6.gpu.torch\n",
        "!wget -q -P $pretrained_path https://github.com/wojtekcz/ml_seminars/releases/download/v0.1/all_tokens.pan_tadeusz.json"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1hgqhWzZwPD"
      },
      "source": [
        "Model Mickiewicz "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVS9kMgOZsUt"
      },
      "source": [
        "!wget -q -P $pretrained_path https://github.com/wojtekcz/ml_seminars/releases/download/v0.1/mickiewicz.h500.l3.e6.gpu.torch\n",
        "!wget -q -P $pretrained_path https://github.com/wojtekcz/ml_seminars/releases/download/v0.1/all_tokens.mickiewicz.json"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT9lQY-BZtQk"
      },
      "source": [
        "Model Witkacy Szewcy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf8XEy5yZO3k"
      },
      "source": [
        "!wget -q -P $pretrained_path https://github.com/wojtekcz/ml_seminars/releases/download/v0.1/witkacy_szewcy.h500.l3.e6.gpu.torch\n",
        "!wget -q -P $pretrained_path https://github.com/wojtekcz/ml_seminars/releases/download/v0.1/all_tokens.witkacy_szewcy.json"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67lBaaQ1gg3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2310145-3d36-4088-dcae-fd939517ef37"
      },
      "source": [
        "#n_epochs = 6 \n",
        "#n_layers = 3\n",
        "#hidden_size = 500 #- uncomment when you want to load existing model \n",
        "\n",
        "modelName = 'pan_tadeusz'  # pan_tadeusz / witkacy_szewcy / mickiewicz\n",
        "ALLTOKS, MODEL = ['all_tokens', 'model']\n",
        "fn_pan_tadeusz = {ALLTOKS: f'all_tokens.{modelName}.json', \n",
        "                  MODEL: f'{modelName}.h{hidden_size}.l{n_layers}.e{n_epochs}.gpu.torch'}\n",
        "fn_dict = fn_pan_tadeusz; fn_dict"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'all_tokens': 'all_tokens.pan_tadeusz.json',\n",
              " 'model': 'pan_tadeusz.h500.l3.e6.gpu.torch'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D9_ONXNMHPK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe201ce-b869-4954-8c35-1182221bf441"
      },
      "source": [
        "if True:\n",
        "  all_tokens_path = pretrained_path / fn_dict[ALLTOKS]\n",
        "  print(f'all_tokens_path = {all_tokens_path}')\n",
        "  all_tokens = json.loads(all_tokens_path.read_text())\n",
        "  \n",
        "  n_characters = len(all_tokens)\n",
        "  tok2idx_dict = {tok: idx for (idx, tok) in enumerate(all_tokens)}\n",
        "\n",
        "  model_path = pretrained_path / fn_dict[MODEL]\n",
        "  decoder = torch.load(model_path)\n",
        "  decoder.gru.flatten_parameters()\n",
        "  print(f'model_path = {model_path}')\n",
        "  print(decoder.state_dict)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_tokens_path = data/rnn_generator/trained/all_tokens.pan_tadeusz.json\n",
            "model_path = data/rnn_generator/trained/pan_tadeusz.h500.l3.e6.gpu.torch\n",
            "<bound method Module.state_dict of RNN(\n",
            "  (encoder): Embedding(5059, 500)\n",
            "  (gru): GRU(500, 500, num_layers=3)\n",
            "  (decoder): Linear(in_features=500, out_features=5059, bias=True)\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fEFWi-FXM5p"
      },
      "source": [
        "## Ewaluacja w różnych \"temperaturach\"\n",
        "\n",
        "W powyższej funkcji `evaluate`, za każdym razem, gdy dokonywana jest prognoza, wyjścia są dzielone przez przekazany argument \"temperature\". Użycie większej liczby sprawia, że wszystkie akcje są bardziej jednakowo prawdopodobne, a tym samym dają nam \"bardziej losowe\" wyniki. Użycie mniejszej wartości (mniejszej niż 1) sprawia, że wysokie prawdopodobieństwa przyczyniają się bardziej. Gdy ustawiamy temperaturę na zero, wybieramy tylko najbardziej prawdopodobne wyjścia.\n",
        "\n",
        "Możemy zobaczyć te efekty poprzez dostosowanie argumentu `temperature`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbRXU5j1LR3_"
      },
      "source": [
        "def print_eval(e_syl):\n",
        "  display(HTML(format_html(fix_punctuation(decode_tokens(syl2str(e_syl, delim=''))))))"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4MTNidlS6j2"
      },
      "source": [
        "prime_tok = str2syl2tok('Litwo! Ojczyzno moja!')"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBm5ibSnXM5p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "fd050ab2-81dd-4cd7-9f97-d84fa08e675d"
      },
      "source": [
        "print_eval(evaluate(prime_tok, 400, temperature=0.8))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Litwo! Ojczyzno moja! \n",
              "<br/>\n",
              "<br/>HASZONIA? \n",
              "<br/>Gerwazy złono, razem, ożeki z Soplica \n",
              "<br/>Odskała, jak blisko stały nawiedzianym omu. \n",
              "<br/>Na przykład pomowny, winset i zaczyna. \n",
              "<br/><font color=\"red\">--</font> Telimena nawet nie poudaki. \n",
              "<br/>Książe Telimena, to tużoników \n",
              "<br/>Stało zakończony powiewając zepienie; \n",
              "<br/>Weprociawszy się w swém oślep Tadeusz z nienama. \n",
              "<br/>Oskałem że miał cię famoty za słońku, \n",
              "<br/>Pan złoczki, powstała i sasają ziołem, \n",
              "<br/>Polowała się s nóg <font color=\"red\">--</font>skie; Panice Gratrzéć, \n",
              "<br/>I zaprzykdziny tak się był to masz świeście, \n",
              "<br/>Zaczęłem ku wiebą na ficzmaszemi władziną. \n",
              "<br/>Ja ja to co mówić Tadeusza ogromie \n",
              "<br/>Słote, i s sobą za nielaszwy jakków widzi. \n",
              "<br/>Szczęściem zgracząc w pole, tagdy nieprowada \n",
              "<br/>Zadal słowa Francuzi, padby odwzrudzili; \n",
              "<br/>Pokoło się, w Litwie tylko zrobiła zomienną; \n",
              "<br/>I wyjąwszy się narzekne; co wczoję konia! \n",
              "<br/>I rzekł w tym zamek, niebiłni szutułem za zabarzcie, \n",
              "<br/>Idzie, Pan Telimena z Litwie Kolołę, \n",
              "<br/>Okoła w złoby, które rodąc że masz cegi \n",
              "<br/>O wiełecznia na Aśćka zagasłe rawicy, \n",
              "<br/>Zwano się Ryś chce uciekł;"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnStt-FzXM5r"
      },
      "source": [
        "Niższe temperatury daja mniejszą różnorodność, wybierając tylko bardziej prawdopodobne wyjścia:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3mkq_sCXM5r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "36214eeb-1a95-4308-aac4-8558f8effee1"
      },
      "source": [
        "print_eval(evaluate(prime_tok, 200, temperature=0.2))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Litwo! Ojczyzno moja! \n",
              "<br/>Ja w Tadeusza, \n",
              "<br/>Czy to Tadeusza? Sędziego Sędziego Widzianie, \n",
              "<br/>Który nieprzyjaciekają się zakłarzcie, \n",
              "<br/>I z Tadeusz niemógł w Tadeusza i Pana Grafie; \n",
              "<br/>Telimena znudzona, i Sędzia nieprzydzianie, \n",
              "<br/>I nie wieczerzy, i z Tadeusza \n",
              "<br/>Postrzegła się, z Tadeusza i Sędziego rana \n",
              "<br/>I z Sędziego, który nieprzydbawali, \n",
              "<br/>Który w oczy Sędziego z Rejent i Pana Pana \n",
              "<br/>W oczyścić, i nie jest podobny Soplicy. \n",
              "<br/>Tak Tadeusz nieraz w Telimeny Pana \n",
              "<br/>Nieszczęja<font color=\"red\">++</font>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l508QP1LXM5t"
      },
      "source": [
        "Wyższe temperatury są bardziej różnorodne, wybierając mniej prawdopodobne wyjścia:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxhfgI-PXM5u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "1fcd36de-de2c-4af4-9911-9399aac80cd0"
      },
      "source": [
        "print_eval(evaluate(prime_tok, 200, temperature=1.4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Litwo! Ojczyzno moja! \n",
              "<br/><font color=\"red\">--</font>Deucza częre z <font color=\"red\">--</font>sze <font color=\"red\">--</font>ną <font color=\"red\">--</font>dzy <font color=\"red\">--</font>wny <font color=\"red\">--</font>ściem błynia <font color=\"red\">--</font>cussłuzeysbach <font color=\"red\">--</font>dzi \n",
              "<br/>Nas stem<font color=\"red\">++</font> klas<font color=\"red\">++</font> wielku<font color=\"red\">++</font> sietewstwa za<font color=\"red\">++</font> chcąc sworase<font color=\"red\">++</font>. \n",
              "<br/>wie<font color=\"red\">++</font> los syt ziemba<font color=\"red\">++</font> ślad <font color=\"red\">--</font>leostobyt<font color=\"red\">++</font> ecy. os brzebodzi: się w kwapadł; końlali. \n",
              "<br/>Ole za <font color=\"red\">--</font>pomwić zwitarum <font color=\"red\">--</font>zy już tj. \n",
              "<br/><font color=\"red\">--</font>Pitéj szarę<font color=\"red\">++</font>, <font color=\"red\">--</font>bum rowinych za pod szotwo zdołem <font color=\"red\">--</font>skim <font color=\"red\">--</font>ru \n",
              "<br/>Wiebę nie <font color=\"red\">--</font>szem <font color=\"red\">--</font>libo zdace, złołem tre<font color=\"red\">++</font> Jali. \n",
              "<br/>Posią góda się <font color=\"red\">--</font>warstrzęś<font color=\"red\">++</font> z; niżwość <font color=\"red\">--</font>chcąc <font color=\"red\">--</font>byżył <font color=\"red\">--</font>deudnie. \n",
              "<br/>w to lub <font color=\"red\">--</font>nierzkie sto <font color=\"red\">--</font>jrzał roznące dach: siwéj nie <font color=\"red\">--</font>Wan<font color=\"red\">++</font> niéj <font color=\"red\">--</font>ńczo<font color=\"red\">++</font> końsze i \n",
              "<br/>A <font color=\"red\">--</font>kiem tych strzelb wygaki_ <font color=\"red\">--</font>tyc<font color=\"red\">++</font> do sterniem ła<font color=\"red\">++</font> gdzie, \n",
              "<br/>Rocię groźkę os<font color=\"red\">++</font> barniach już wawdząc <font color=\"red\">--</font>ka ku <font color=\"red\">--</font>żył że swa<font color=\"red\">++</font>. \n",
              "<br/>\n",
              "<br/>Sień zwłasach <font color=\"red\">--</font>ctwem <font color=\"red\">--</font>sta<font color=\"red\">++</font> wiodł <font color=\"red\">--</font>szę zieszło le<font color=\"red\">++</font>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UQoZYk2XM5v"
      },
      "source": [
        "## Ćwiczenia\n",
        "\n",
        "* Trenuj z własnym zestawem danych, np.\n",
        "     * Tekst od innego autora\n",
        "     * Posty na blogu\n",
        "     * Kody źródłowe\n",
        "* Zwiększ liczbę warstw i rozmiar sieci, aby uzyskać lepsze wyniki"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFHHvkI6XM5w"
      },
      "source": [
        "**Następnie**: [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tJDT2lCceZL"
      },
      "source": [
        "## (debug) Monitorowanie maszyny wirtualnej"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKdCiolHcg4e"
      },
      "source": [
        "def print_memsize():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(f'{process.memory_info().rss / 1024**3:.5} GB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diBUUyOociRy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00aa5912-9fdb-49a2-dffd-75ec70d71c0b"
      },
      "source": [
        "print_memsize()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.751 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuHoo0ARtjz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15704364-052c-4690-fdea-d2cb3c35408d"
      },
      "source": [
        "!uptime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 12:12:31 up 37 min,  0 users,  load average: 0.07, 0.09, 0.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-NsNCAaq89c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}